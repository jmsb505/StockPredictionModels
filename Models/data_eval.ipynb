{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pylab as rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional,GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from  sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import warnings\n",
    "from tcn import TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on TSLA...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import seaborn as sns\n",
    "import pylab as rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional,GRU\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from  sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import warnings\n",
    "from tcn import TCN\n",
    "\n",
    "# %% [markdown]\n",
    "# \n",
    "arch='bitcn'\n",
    "project_path='/home/j/usfq/Proyecto Integrador/StockPredictionModels/Data'\n",
    "# %%\n",
    "df=pd.read_csv(project_path+'/Data/Complete.csv')\n",
    "df\n",
    "\n",
    "# %%\n",
    "#turn date into unix time\n",
    "#df['Date'] = pd.to_datetime(df['Date'])\n",
    "#df['Date'] = df['Date'].apply(lambda x: x.timestamp())\n",
    "#df\n",
    "\n",
    "# %%\n",
    "#generate new dataframes for each ticker_symbol]\n",
    "metric_labels=['Testing-MSE','Validation-MSE','testing-MAE','validation-MAE','testing-mape','validation-mape','testing-RMSE','validation-RMSE', 'testing-MPE','validation-MPE']\n",
    "metrics_df=pd.DataFrame()\n",
    "metrics_df['Metrics']=metric_labels\n",
    "std_metrics_df=pd.DataFrame()\n",
    "std_metrics_df['Metrics']=metric_labels\n",
    "#save dataframe as csv\n",
    "metrics_df.to_csv(project_path+f'/Results/{arch}_metrics.csv',index=False)\n",
    "std_metrics_df.to_csv(project_path+f'/Results/{arch}_std_metrics.csv',index=False)\n",
    "metrics=[]\n",
    "df_dict={}\n",
    "for key in df['ticker_symbol'].unique():\n",
    "    df_dict[key]=df[df['ticker_symbol']==key]\n",
    "    df_dict[key]=df_dict[key].drop(columns=['ticker_symbol'])\n",
    "    df_dict[key]=df_dict[key].sort_values(by=['Date']).reset_index(drop=True)\n",
    "    #df_dict[key]=df_dict[key].drop(columns=['Date'])\n",
    "mse_t_p=[]\n",
    "mae_t_p=[]\n",
    "mape_t_p=[]\n",
    "rmse_t_p=[]\n",
    "mpe_t_p=[]\n",
    "mse_v_p=[]\n",
    "mae_v_p=[]\n",
    "mape_v_p=[]\n",
    "rmse_v_p=[]\n",
    "mpe_v_p=[]\n",
    "\n",
    "overall_mse_train=[]\n",
    "overall_mse_val=[]\n",
    "overall_std_train=[]\n",
    "overall_std_val=[]\n",
    "\n",
    "keys=df_dict.keys()\n",
    "#to list\n",
    "keys_list=list(keys)\n",
    "\n",
    "\n",
    "# %%\n",
    "ticker='TSLA'\n",
    "print(f'Working on {ticker}...')\n",
    "# %%\n",
    "df=df_dict[ticker].copy()\n",
    "\n",
    "\n",
    "# %%\n",
    "#putting the close column on the last position\n",
    "df=df[['Date', 'p_sentiment', 'Open', 'High', 'Low',\n",
    "    'Volume', 'unrate', 'psr', 'm2', 'dspic', 'pce', 'reer', 'ir', 'ffer',\n",
    "    'tcs', 'indpro', 'ccpi', 'Close']]\n",
    "\n",
    "# %%\n",
    "dates = pd.to_datetime(df['Date'])\n",
    "\n",
    "# %%\n",
    "cols=list(df)[1:]\n",
    "\n",
    "\n",
    "# %%\n",
    "df_for_training = df[cols].astype(float)\n",
    "\n",
    "# %%\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df_for_training)\n",
    "\n",
    "# %%\n",
    "#split scaled data into training, val and testing\n",
    "#train_data=scaled_data[0:1000,:]\n",
    "#val_data=scaled_data[1000:1125,:]\n",
    "#test_data=scaled_data[1125:,:]\n",
    "\n",
    "# %%\n",
    "n_future = 1 # Number of days we want to predict into the future\n",
    "n_past = 7 # Number of past days we want to use to predict the future\n",
    "\n",
    "# %%\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(n_past, len(scaled_data) - n_future +1):\n",
    "    X.append(scaled_data[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    y.append(scaled_data[i + n_future - 1:i + n_future, len(cols)-1])\n",
    "\n",
    "# %%\n",
    "#shape of X_s and y_s\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77183473 0.24678828 0.21948349 0.23172104 0.14147722 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.23121105]\n",
      " [0.62544497 0.23144731 0.21127918 0.22138103 0.16859786 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.23535347]\n",
      " [0.60139272 0.24268824 0.21334812 0.24090432 0.0686182  1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.23420473]\n",
      " [0.7053284  0.24084322 0.20985233 0.24171053 0.08301029 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.23305598]\n",
      " [0.61846633 0.22755228 0.19622605 0.22400984 0.12022148 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.21927104]\n",
      " [0.6895419  0.20749626 0.17657131 0.2039958  0.15913872 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.20378043]\n",
      " [0.51363156 0.20841877 0.18777203 0.20981426 0.11442336 1.\n",
      "  0.6        0.         0.         0.         0.         0.22902763\n",
      "  0.00178319 0.         0.77924797 0.         0.21088176]]\n"
     ]
    }
   ],
   "source": [
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21088176]\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
